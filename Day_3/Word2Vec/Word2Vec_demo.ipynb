{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "\n",
       "div.cell { /* Tunes the space between cells */\n",
       "margin-top:1em;\n",
       "margin-bottom:1em;\n",
       "}\n",
       "\n",
       "div.text_cell_render h1 { /* Main titles bigger, centered */\n",
       "font-size: 2.0em;\n",
       "line-height:1.6em;\n",
       "text-align:center;\n",
       "}\n",
       "\n",
       "div.text_cell_render h2 { /*  Parts names nearer from text */\n",
       "margin-bottom: 1em;\n",
       "text-align:center;\n",
       "}\n",
       "\n",
       "\n",
       "div.text_cell_render { /* Customize text cells */\n",
       "font-family: 'Times New Roman';\n",
       "font-size:1.2em;\n",
       "line-height:1.2em;\n",
       "padding-left:1em;\n",
       "padding-right:3em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "HTML(\"\"\"\n",
    "<style>\n",
    "\n",
    "div.cell { /* Tunes the space between cells */\n",
    "margin-top:1em;\n",
    "margin-bottom:1em;\n",
    "}\n",
    "\n",
    "div.text_cell_render h1 { /* Main titles bigger, centered */\n",
    "font-size: 2.0em;\n",
    "line-height:1.6em;\n",
    "text-align:center;\n",
    "}\n",
    "\n",
    "div.text_cell_render h2 { /*  Parts names nearer from text */\n",
    "margin-bottom: 1em;\n",
    "text-align:center;\n",
    "}\n",
    "\n",
    "\n",
    "div.text_cell_render { /* Customize text cells */\n",
    "font-family: 'Times New Roman';\n",
    "font-size:1.2em;\n",
    "line-height:1.2em;\n",
    "padding-left:1em;\n",
    "padding-right:3em;\n",
    "}\n",
    "</style>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec model demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import models\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## King and Queen example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector value of 'King - man + woman:\n",
      "[('queen', 0.7118192315101624), ('monarch', 0.6189672946929932), ('princess', 0.5902429819107056), ('crown_prince', 0.5499460697174072), ('prince', 0.5377322435379028), ('kings', 0.5236843824386597), ('Queen_Consort', 0.5235944986343384), ('queens', 0.5181134343147278), ('sultan', 0.5098592638969421), ('monarchy', 0.5087411999702454)]\n",
      "\n",
      "--------------------------------\n",
      "\n",
      "Similarity between man and woman: 0.76640%\n",
      "Similarity between man and woman in percentage: 76.64012%\n"
     ]
    }
   ],
   "source": [
    "# My desktop path: /home/jalaj/MLdataset/models\n",
    "# My laptop path : /home/jalaj/ML_datasets/models/\n",
    "w = models.Word2Vec.load_word2vec_format('/home/jalaj/MLdataset/models/GoogleNews-vectors-negative300.bin', binary=True)\n",
    "\n",
    "print(\"Vector value of 'King - man + woman:\")\n",
    "print (w.wv.most_similar(positive=['woman', 'king'], negative=['man']))\n",
    "print()\n",
    "\n",
    "print ('--------------------------------')\n",
    "\n",
    "print()\n",
    "print('Similarity between man and woman: ' + '{:.5f}%'.format(w.similarity('woman', 'man')))\n",
    "print('Similarity between man and woman in percentage: ' + '{:.5f}%'.format(w.similarity('woman', 'man')*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector for word 'the' \n",
      "\n",
      "[ 0.08007812  0.10498047  0.04980469  0.0534668  -0.06738281 -0.12060547\n",
      "  0.03515625 -0.11865234  0.04394531  0.03015137 -0.05688477 -0.07617188\n",
      "  0.01287842  0.04980469 -0.08496094 -0.06347656  0.00628662 -0.04321289\n",
      "  0.02026367  0.01330566 -0.01953125  0.09277344 -0.171875   -0.00131989\n",
      "  0.06542969  0.05834961 -0.08251953  0.0859375  -0.00318909  0.05859375\n",
      " -0.03491211 -0.0123291  -0.0480957  -0.00302124  0.05639648  0.01495361\n",
      " -0.07226562 -0.05224609  0.09667969  0.04296875 -0.03540039 -0.07324219\n",
      "  0.03271484 -0.06176758  0.00787354  0.0035553  -0.00878906  0.0390625\n",
      "  0.03833008  0.04443359  0.06982422  0.01263428 -0.00445557 -0.03320312\n",
      " -0.04272461  0.09765625 -0.02160645 -0.0378418   0.01190186 -0.01391602\n",
      " -0.11328125  0.09326172 -0.03930664 -0.11621094  0.02331543 -0.01599121\n",
      "  0.02636719  0.10742188 -0.00466919  0.09619141  0.0279541  -0.05395508\n",
      "  0.08544922 -0.03686523 -0.02026367 -0.08544922  0.125       0.14453125\n",
      "  0.0267334   0.15039062  0.05273438 -0.18652344  0.08154297 -0.01062012\n",
      " -0.03735352 -0.07324219 -0.07519531  0.03613281 -0.13183594  0.00616455\n",
      "  0.05078125  0.04516602  0.0100708  -0.15039062 -0.06005859  0.05761719\n",
      " -0.00692749  0.01586914 -0.0213623   0.10351562 -0.00029182 -0.046875\n",
      " -0.01635742 -0.07861328 -0.06933594  0.01635742 -0.03149414 -0.01373291\n",
      " -0.03662109 -0.08886719 -0.0480957  -0.01318359 -0.07177734  0.00588989\n",
      " -0.04614258  0.03979492  0.10058594 -0.04931641  0.07568359  0.03881836\n",
      " -0.16699219 -0.09619141 -0.10107422  0.02905273 -0.05786133 -0.01928711\n",
      " -0.04296875 -0.08398438 -0.01989746  0.05151367  0.00848389 -0.03613281\n",
      " -0.14941406 -0.01855469 -0.03637695 -0.07666016 -0.03955078 -0.06152344\n",
      " -0.02001953  0.04150391  0.03686523 -0.07226562  0.00592041 -0.06298828\n",
      "  0.00738525 -0.01586914  0.01611328 -0.01452637  0.00772095  0.10107422\n",
      " -0.00558472  0.01428223 -0.07617188  0.05639648 -0.01293945  0.03063965\n",
      " -0.02490234 -0.09863281  0.0324707  -0.02807617 -0.08105469  0.02062988\n",
      "  0.01611328 -0.04199219 -0.03491211 -0.03759766  0.05493164  0.01373291\n",
      "  0.02685547 -0.05859375 -0.07177734 -0.12011719 -0.02282715 -0.1640625\n",
      " -0.00361633 -0.05981445  0.07080078 -0.07714844  0.05175781 -0.04296875\n",
      " -0.04833984  0.0300293  -0.06591797 -0.03173828 -0.04882812 -0.03491211\n",
      "  0.05883789 -0.01464844  0.18066406  0.05688477  0.05249023  0.05786133\n",
      "  0.11669922  0.05200195 -0.0534668   0.01867676 -0.015625    0.00576782\n",
      " -0.07324219 -0.11621094  0.04052734  0.0625     -0.04321289  0.01055908\n",
      "  0.02172852  0.04248047  0.03271484  0.04418945  0.05761719  0.02612305\n",
      " -0.01831055 -0.02697754 -0.00674438  0.00509644 -0.11621094  0.00364685\n",
      "  0.05761719 -0.05957031 -0.08837891  0.0135498   0.04541016 -0.04638672\n",
      " -0.0177002  -0.0625      0.03442383 -0.02416992  0.03088379  0.09570312\n",
      "  0.07958984  0.03930664  0.0279541  -0.0859375   0.08105469  0.06640625\n",
      " -0.00041962 -0.06933594  0.03588867 -0.03417969  0.04492188 -0.00772095\n",
      " -0.00741577 -0.04760742  0.01397705 -0.09960938  0.0246582  -0.09960938\n",
      "  0.11474609  0.03173828  0.02209473  0.07226562  0.03686523  0.02563477\n",
      "  0.01367188 -0.02734375  0.00592041 -0.06738281  0.05053711 -0.02832031\n",
      " -0.04516602 -0.01733398  0.02111816  0.03515625 -0.04296875  0.06640625\n",
      "  0.12207031  0.12353516  0.0039978   0.04516602 -0.01855469  0.04833984\n",
      "  0.04516602  0.08691406  0.02941895  0.03759766  0.03442383 -0.07373047\n",
      " -0.0402832  -0.14648438 -0.02441406 -0.01953125  0.0065918  -0.0018158\n",
      " -0.01092529  0.09326172  0.06542969  0.01843262 -0.09326172 -0.01574707\n",
      " -0.07128906 -0.08935547 -0.07128906 -0.03015137 -0.01300049  0.01635742\n",
      " -0.01831055  0.01483154  0.00500488  0.00366211  0.04760742 -0.06884766]\n",
      "Vocabulary doesn't include word 'a'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# My desktop path: /home/jalaj/MLdataset/models\n",
    "# My laptop path : /home/jalaj/ML_datasets/models/\n",
    "w =  models.Word2Vec.load_word2vec_format('/home/jalaj/MLdataset/models/GoogleNews-vectors-negative300.bin', binary=True)\n",
    "if 'the' in w.wv.vocab:\n",
    "    print (\"Vector for word 'the' \\n\")\n",
    "    print (w.wv['the'])\n",
    "else:\n",
    "    print (\"Vocabulary doesn't include word 'the'\\n\")\n",
    "if 'a' in w.wv.vocab:\n",
    "    print (\"Vector for word 'a' \\n\")\n",
    "    print (w.wv['a'])\n",
    "else:\n",
    "    print (\"Vocabulary doesn't include word 'a'\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Glove example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from gensim.models.word2vec import Text8Corpus\n",
    "from glove import Corpus, Glove"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download text8 corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2018-06-28 14:43:43--  http://mattmahoney.net/dc/text8.zip\n",
      "Resolving mattmahoney.net (mattmahoney.net)... 67.195.197.75\n",
      "Connecting to mattmahoney.net (mattmahoney.net)|67.195.197.75|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 31344016 (30M) [application/zip]\n",
      "Saving to: ‘/tmp/text8.zip’\n",
      "\n",
      "text8.zip           100%[===================>]  29.89M  49.1KB/s    in 2m 58s  \n",
      "\n",
      "2018-06-28 14:46:41 (172 KB/s) - ‘/tmp/text8.zip’ saved [31344016/31344016]\n",
      "\n",
      "Archive:  /tmp/text8.zip\n",
      "  inflating: text8                   \n"
     ]
    }
   ],
   "source": [
    "# for installing text8 corpus you should follow this commands\n",
    "\n",
    "#!wget http://mattmahoney.net/dc/text8.zip -P /tmp\n",
    "#!sudo unzip /tmp/text8.zip\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get glove vector for words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing 30 training epochs with 4 threads\n",
      "Epoch 0\n",
      "Epoch 1\n",
      "Epoch 2\n",
      "Epoch 3\n",
      "Epoch 4\n",
      "Epoch 5\n",
      "Epoch 6\n",
      "Epoch 7\n",
      "Epoch 8\n",
      "Epoch 9\n",
      "Epoch 10\n",
      "Epoch 11\n",
      "Epoch 12\n",
      "Epoch 13\n",
      "Epoch 14\n",
      "Epoch 15\n",
      "Epoch 16\n",
      "Epoch 17\n",
      "Epoch 18\n",
      "Epoch 19\n",
      "Epoch 20\n",
      "Epoch 21\n",
      "Epoch 22\n",
      "Epoch 23\n",
      "Epoch 24\n",
      "Epoch 25\n",
      "Epoch 26\n",
      "Epoch 27\n",
      "Epoch 28\n",
      "Epoch 29\n",
      "[('frontalis', 0.7144487978313968), ('giant', 0.6884460325523899), ('mysterious', 0.6800810866092589), ('dome', 0.6615427133448449), ('snake', 0.6584393949988718), ('stenella', 0.6560163293941425), ('panda', 0.6492812323402911), ('cloud', 0.644848994496498), ('moth', 0.6369861202713943)]\n",
      "[('young', 0.7677979028354031), ('woman', 0.726566078321198), ('man', 0.7226530174460424), ('baby', 0.7049971091919205), ('girls', 0.6923263638274243), ('teenage', 0.6762847418361446), ('boy', 0.6731077079484508), ('wise', 0.6652096177384232), ('beautiful', 0.648401594432735)]\n",
      "[('driver', 0.8757231267013678), ('race', 0.8617780069563485), ('taxi', 0.7439661235453976), ('crash', 0.72462708407229), ('cars', 0.7061833190246786), ('racing', 0.7042829669399977), ('touring', 0.6811196791445745), ('accident', 0.6788719720727868), ('truck', 0.6335326166683497)]\n"
     ]
    }
   ],
   "source": [
    "#sentences = list(itertools.islice(Text8Corpus('/tmp/text8'), None))\n",
    "sentences = list(itertools.islice(Text8Corpus('./text8'), None))\n",
    "corpus = Corpus()\n",
    "corpus.fit(sentences, window=10)\n",
    "glove = Glove(no_components=100, learning_rate=0.05)\n",
    "glove.fit(corpus.matrix, epochs=30, no_threads=4, verbose=True)\n",
    "glove.add_dictionary(corpus.dictionary)\n",
    "\n",
    "print (glove.most_similar('frog', number=10))\n",
    "print (glove.most_similar('girl', number=10))\n",
    "print (glove.most_similar('car', number=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doc2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python example to train doc2vec model (with or without pre-trained word embeddings)\n",
    "import logging\n",
    "import gensim.models as g\n",
    "import codecs\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc2vec parameters\n",
    "vector_size = 300\n",
    "window_size = 15\n",
    "min_count = 1\n",
    "sampling_threshold = 1e-5\n",
    "negative_size = 5\n",
    "train_epoch = 100\n",
    "dm = 0  # 0 = dbow; 1 = dmpv\n",
    "worker_count = 1  # number of parallel processes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-07-01 17:08:30,422 : INFO : collecting all words and their counts\n",
      "2018-07-01 17:08:30,480 : INFO : PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags\n",
      "2018-07-01 17:08:30,571 : INFO : collected 11097 word types and 1000 unique tags from a corpus of 1000 examples and 84408 words\n",
      "2018-07-01 17:08:30,573 : INFO : Loading a fresh vocabulary\n",
      "2018-07-01 17:08:30,762 : INFO : min_count=1 retains 11097 unique words (100% of original 11097, drops 0)\n",
      "2018-07-01 17:08:30,763 : INFO : min_count=1 leaves 84408 word corpus (100% of original 84408, drops 0)\n",
      "2018-07-01 17:08:30,787 : INFO : deleting the raw counts dictionary of 11097 items\n",
      "2018-07-01 17:08:30,788 : INFO : sample=1e-05 downsamples 3599 most-common words\n",
      "2018-07-01 17:08:30,789 : INFO : downsampling leaves estimated 22704 word corpus (26.9% of prior 84408)\n",
      "2018-07-01 17:08:30,789 : INFO : estimated required memory for 11097 words and 300 dimensions: 33381300 bytes\n",
      "2018-07-01 17:08:30,806 : INFO : resetting layer weights\n",
      "2018-07-01 17:08:30,916 : INFO : training model with 1 workers on 11098 vocabulary and 300 features, using sg=1 hs=0 sample=1e-05 negative=5 window=15\n",
      "2018-07-01 17:08:30,917 : INFO : expecting 1000 sentences, matching count from corpus used for vocabulary survey\n",
      "2018-07-01 17:08:31,965 : INFO : PROGRESS: at 1.76% examples, 39810 words/s, in_qsize 1, out_qsize 0\n",
      "2018-07-01 17:08:32,987 : INFO : PROGRESS: at 3.54% examples, 40493 words/s, in_qsize 1, out_qsize 0\n",
      "2018-07-01 17:08:34,032 : INFO : PROGRESS: at 5.29% examples, 40247 words/s, in_qsize 1, out_qsize 0\n",
      "2018-07-01 17:08:35,071 : INFO : PROGRESS: at 7.04% examples, 40230 words/s, in_qsize 1, out_qsize 0\n",
      "2018-07-01 17:08:36,128 : INFO : PROGRESS: at 8.96% examples, 40610 words/s, in_qsize 1, out_qsize 0\n",
      "2018-07-01 17:08:37,153 : INFO : PROGRESS: at 10.83% examples, 41077 words/s, in_qsize 1, out_qsize 0\n",
      "2018-07-01 17:08:38,187 : INFO : PROGRESS: at 12.71% examples, 41384 words/s, in_qsize 1, out_qsize 0\n",
      "2018-07-01 17:08:39,215 : INFO : PROGRESS: at 14.59% examples, 41649 words/s, in_qsize 1, out_qsize 0\n",
      "2018-07-01 17:08:40,223 : INFO : PROGRESS: at 16.36% examples, 41627 words/s, in_qsize 1, out_qsize 0\n",
      "2018-07-01 17:08:41,265 : INFO : PROGRESS: at 18.23% examples, 41732 words/s, in_qsize 1, out_qsize 0\n",
      "2018-07-01 17:08:42,319 : INFO : PROGRESS: at 20.00% examples, 41511 words/s, in_qsize 2, out_qsize 0\n",
      "2018-07-01 17:08:43,324 : INFO : PROGRESS: at 21.64% examples, 41303 words/s, in_qsize 1, out_qsize 0\n",
      "2018-07-01 17:08:44,368 : INFO : PROGRESS: at 23.43% examples, 41225 words/s, in_qsize 1, out_qsize 0\n",
      "2018-07-01 17:08:45,431 : INFO : PROGRESS: at 25.29% examples, 41273 words/s, in_qsize 1, out_qsize 0\n",
      "2018-07-01 17:08:46,458 : INFO : PROGRESS: at 27.14% examples, 41405 words/s, in_qsize 1, out_qsize 0\n",
      "2018-07-01 17:08:47,481 : INFO : PROGRESS: at 29.04% examples, 41535 words/s, in_qsize 1, out_qsize 0\n",
      "2018-07-01 17:08:48,506 : INFO : PROGRESS: at 30.96% examples, 41651 words/s, in_qsize 1, out_qsize 0\n",
      "2018-07-01 17:08:49,536 : INFO : PROGRESS: at 32.83% examples, 41744 words/s, in_qsize 1, out_qsize 0\n",
      "2018-07-01 17:08:50,581 : INFO : PROGRESS: at 34.72% examples, 41801 words/s, in_qsize 2, out_qsize 0\n",
      "2018-07-01 17:08:51,613 : INFO : PROGRESS: at 36.48% examples, 41753 words/s, in_qsize 2, out_qsize 0\n",
      "2018-07-01 17:08:52,622 : INFO : PROGRESS: at 38.23% examples, 41733 words/s, in_qsize 1, out_qsize 0\n",
      "2018-07-01 17:08:53,652 : INFO : PROGRESS: at 40.00% examples, 41668 words/s, in_qsize 1, out_qsize 0\n",
      "2018-07-01 17:08:54,692 : INFO : PROGRESS: at 41.65% examples, 41493 words/s, in_qsize 1, out_qsize 0\n",
      "2018-07-01 17:08:55,701 : INFO : PROGRESS: at 43.29% examples, 41380 words/s, in_qsize 1, out_qsize 0\n",
      "2018-07-01 17:08:56,758 : INFO : PROGRESS: at 45.04% examples, 41305 words/s, in_qsize 2, out_qsize 0\n",
      "2018-07-01 17:08:57,772 : INFO : PROGRESS: at 46.83% examples, 41300 words/s, in_qsize 1, out_qsize 0\n",
      "2018-07-01 17:08:58,815 : INFO : PROGRESS: at 48.59% examples, 41266 words/s, in_qsize 1, out_qsize 0\n",
      "2018-07-01 17:08:59,828 : INFO : PROGRESS: at 50.36% examples, 41267 words/s, in_qsize 1, out_qsize 0\n",
      "2018-07-01 17:09:00,849 : INFO : PROGRESS: at 52.10% examples, 41251 words/s, in_qsize 1, out_qsize 0\n",
      "2018-07-01 17:09:01,859 : INFO : PROGRESS: at 53.89% examples, 41253 words/s, in_qsize 1, out_qsize 0\n",
      "2018-07-01 17:09:02,862 : INFO : PROGRESS: at 55.65% examples, 41275 words/s, in_qsize 1, out_qsize 0\n",
      "2018-07-01 17:09:03,880 : INFO : PROGRESS: at 57.54% examples, 41355 words/s, in_qsize 1, out_qsize 0\n",
      "2018-07-01 17:09:04,899 : INFO : PROGRESS: at 59.43% examples, 41432 words/s, in_qsize 1, out_qsize 0\n",
      "2018-07-01 17:09:05,918 : INFO : PROGRESS: at 61.29% examples, 41501 words/s, in_qsize 1, out_qsize 0\n",
      "2018-07-01 17:09:06,926 : INFO : PROGRESS: at 63.14% examples, 41570 words/s, in_qsize 1, out_qsize 0\n",
      "2018-07-01 17:09:07,932 : INFO : PROGRESS: at 65.04% examples, 41645 words/s, in_qsize 1, out_qsize 0\n",
      "2018-07-01 17:09:08,939 : INFO : PROGRESS: at 66.96% examples, 41718 words/s, in_qsize 1, out_qsize 0\n",
      "2018-07-01 17:09:09,948 : INFO : PROGRESS: at 68.83% examples, 41777 words/s, in_qsize 1, out_qsize 0\n",
      "2018-07-01 17:09:11,003 : INFO : PROGRESS: at 70.72% examples, 41798 words/s, in_qsize 1, out_qsize 0\n",
      "2018-07-01 17:09:12,055 : INFO : PROGRESS: at 72.59% examples, 41820 words/s, in_qsize 2, out_qsize 0\n",
      "2018-07-01 17:09:13,095 : INFO : PROGRESS: at 74.48% examples, 41853 words/s, in_qsize 2, out_qsize 0\n",
      "2018-07-01 17:09:14,131 : INFO : PROGRESS: at 76.36% examples, 41881 words/s, in_qsize 2, out_qsize 0\n",
      "2018-07-01 17:09:15,152 : INFO : PROGRESS: at 78.23% examples, 41917 words/s, in_qsize 1, out_qsize 0\n",
      "2018-07-01 17:09:16,225 : INFO : PROGRESS: at 80.10% examples, 41907 words/s, in_qsize 1, out_qsize 0\n",
      "2018-07-01 17:09:17,240 : INFO : PROGRESS: at 81.89% examples, 41890 words/s, in_qsize 1, out_qsize 0\n",
      "2018-07-01 17:09:18,241 : INFO : PROGRESS: at 83.64% examples, 41896 words/s, in_qsize 1, out_qsize 0\n",
      "2018-07-01 17:09:19,301 : INFO : PROGRESS: at 85.54% examples, 41904 words/s, in_qsize 1, out_qsize 0\n",
      "2018-07-01 17:09:20,305 : INFO : PROGRESS: at 87.29% examples, 41899 words/s, in_qsize 2, out_qsize 0\n",
      "2018-07-01 17:09:21,311 : INFO : PROGRESS: at 89.04% examples, 41890 words/s, in_qsize 1, out_qsize 0\n",
      "2018-07-01 17:09:22,361 : INFO : PROGRESS: at 90.96% examples, 41903 words/s, in_qsize 1, out_qsize 0\n",
      "2018-07-01 17:09:23,380 : INFO : PROGRESS: at 92.72% examples, 41889 words/s, in_qsize 1, out_qsize 0\n",
      "2018-07-01 17:09:24,402 : INFO : PROGRESS: at 94.48% examples, 41878 words/s, in_qsize 1, out_qsize 0\n",
      "2018-07-01 17:09:25,408 : INFO : PROGRESS: at 96.23% examples, 41869 words/s, in_qsize 2, out_qsize 0\n",
      "2018-07-01 17:09:26,460 : INFO : PROGRESS: at 98.10% examples, 41877 words/s, in_qsize 2, out_qsize 0\n",
      "2018-07-01 17:09:27,507 : INFO : PROGRESS: at 100.00% examples, 41893 words/s, in_qsize 0, out_qsize 1\n",
      "2018-07-01 17:09:27,508 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-07-01 17:09:27,508 : INFO : training on 8440800 raw words (2370696 effective words) took 56.6s, 41892 effective words/s\n",
      "2018-07-01 17:09:27,509 : INFO : saving Doc2Vec object under ./doc2vecdata/model_new.bin, separately None\n",
      "2018-07-01 17:09:27,509 : INFO : not storing attribute cum_table\n",
      "2018-07-01 17:09:27,511 : INFO : not storing attribute syn0norm\n",
      "2018-07-01 17:09:27,821 : INFO : saved ./doc2vecdata/model_new.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training is over....!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# pretrained word embeddings\n",
    "pretrained_emb = \"./doc2vecdata/pretrained_word_embeddings.txt\"\n",
    "\n",
    "# None if use without pretrained embeddings\n",
    "\n",
    "# input corpus\n",
    "train_corpus = \"./doc2vecdata/train_docs.txt\"\n",
    "\n",
    "# output model\n",
    "saved_path = \"./doc2vecdata/model_new.bin\"\n",
    "\n",
    "# enable logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "# train doc2vec model\n",
    "docs = g.doc2vec.TaggedLineDocument(train_corpus)\n",
    "model = g.Doc2Vec(docs, size=vector_size, window=window_size, min_count=min_count, sample=sampling_threshold,\n",
    "                  workers=worker_count, hs=0, dm=dm, negative=negative_size, dbow_words=1, dm_concat=1,\n",
    "                  iter=train_epoch)\n",
    "\n",
    "# save model\n",
    "model.save(saved_path)\n",
    "\n",
    "print (\"training is over....!\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-07-01 17:09:27,844 : INFO : loading Doc2Vec object from ./doc2vecdata/model_new.bin\n",
      "2018-07-01 17:09:28,018 : INFO : loading wv recursively from ./doc2vecdata/model_new.bin.wv.* with mmap=None\n",
      "2018-07-01 17:09:28,019 : INFO : loading docvecs recursively from ./doc2vecdata/model_new.bin.docvecs.* with mmap=None\n",
      "2018-07-01 17:09:28,020 : INFO : setting ignored attribute cum_table to None\n",
      "2018-07-01 17:09:28,020 : INFO : setting ignored attribute syn0norm to None\n",
      "2018-07-01 17:09:28,021 : INFO : loaded ./doc2vecdata/model_new.bin\n",
      "2018-07-01 17:09:28,034 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing started....!\n",
      "\n",
      "[('harp', 0.7706708908081055), ('clingstone', 0.769257128238678), ('plum', 0.7686954736709595), ('tow', 0.7682487964630127), ('burlap', 0.7681158781051636), ('bag', 0.7679807543754578), ('andirons', 0.7679640650749207), ('peach', 0.7661047577857971), ('harmonica', 0.765450119972229), ('dragonfly', 0.765364408493042)]\n"
     ]
    }
   ],
   "source": [
    "print (\"testing started....!\\n\")\n",
    "#parameters\n",
    "model=\"./doc2vecdata/model_new.bin\"\n",
    "test_docs=\"./doc2vecdata/test_docs.txt\"\n",
    "output_file=\"./test_vectors_new.txt\"\n",
    "\n",
    "#inference hyper-parameters\n",
    "start_alpha=0.01\n",
    "infer_epoch=1000\n",
    "\n",
    "#load model\n",
    "m = g.Doc2Vec.load(model)\n",
    "print (m.wv.most_similar(positive=['family', 'dog']))\n",
    "test_docs = [ x.strip().split() for x in codecs.open(test_docs, \"r\", \"utf-8\").readlines() ]\n",
    "\n",
    "#infer test vectors\n",
    "output = open(output_file, \"w\")\n",
    "for d in test_docs:\n",
    "    output.write( \" \".join([str(x) for x in m.infer_vector(d, alpha=start_alpha, steps=infer_epoch)]) + \"\\n\" )\n",
    "output.flush()\n",
    "output.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
