{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "\n",
       "div.cell { /* Tunes the space between cells */\n",
       "margin-top:1em;\n",
       "margin-bottom:1em;\n",
       "}\n",
       "\n",
       "div.text_cell_render h1 { /* Main titles bigger, centered */\n",
       "font-size: 2.0em;\n",
       "line-height:1.6em;\n",
       "text-align:center;\n",
       "}\n",
       "\n",
       "div.text_cell_render h2 { /*  Parts names nearer from text */\n",
       "margin-bottom: 1em;\n",
       "text-align:center;\n",
       "}\n",
       "\n",
       "\n",
       "div.text_cell_render { /* Customize text cells */\n",
       "font-family: 'Times New Roman';\n",
       "font-size:1.2em;\n",
       "line-height:1.2em;\n",
       "padding-left:1em;\n",
       "padding-right:3em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "HTML(\"\"\"\n",
    "<style>\n",
    "\n",
    "div.cell { /* Tunes the space between cells */\n",
    "margin-top:1em;\n",
    "margin-bottom:1em;\n",
    "}\n",
    "\n",
    "div.text_cell_render h1 { /* Main titles bigger, centered */\n",
    "font-size: 2.0em;\n",
    "line-height:1.6em;\n",
    "text-align:center;\n",
    "}\n",
    "\n",
    "div.text_cell_render h2 { /*  Parts names nearer from text */\n",
    "margin-bottom: 1em;\n",
    "text-align:center;\n",
    "}\n",
    "\n",
    "\n",
    "div.text_cell_render { /* Customize text cells */\n",
    "font-family: 'Times New Roman';\n",
    "font-size:1.2em;\n",
    "line-height:1.2em;\n",
    "padding-left:1em;\n",
    "padding-right:3em;\n",
    "}\n",
    "</style>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec model demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import models\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## King and Queen example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector value of 'King - man + woman:\n",
      "[('queen', 0.7118192315101624), ('monarch', 0.6189672946929932), ('princess', 0.5902429819107056), ('crown_prince', 0.5499460697174072), ('prince', 0.5377322435379028), ('kings', 0.5236843824386597), ('Queen_Consort', 0.5235944986343384), ('queens', 0.5181134343147278), ('sultan', 0.5098592638969421), ('monarchy', 0.5087411999702454)]\n",
      "\n",
      "--------------------------------\n",
      "\n",
      "Similarity between man and woman: 0.76640%\n",
      "Similarity between man and woman in percentage: 76.64012%\n"
     ]
    }
   ],
   "source": [
    "# My desktop path: /home/jalaj/MLdataset/models\n",
    "# My laptop path : /home/jalaj/ML_datasets/models/\n",
    "w = models.Word2Vec.load_word2vec_format('/home/jalaj/MLdataset/models/GoogleNews-vectors-negative300.bin', binary=True)\n",
    "\n",
    "print(\"Vector value of 'King - man + woman:\")\n",
    "print (w.wv.most_similar(positive=['woman', 'king'], negative=['man']))\n",
    "print()\n",
    "\n",
    "print ('--------------------------------')\n",
    "\n",
    "print()\n",
    "print('Similarity between man and woman: ' + '{:.5f}%'.format(w.similarity('woman', 'man')))\n",
    "print('Similarity between man and woman in percentage: ' + '{:.5f}%'.format(w.similarity('woman', 'man')*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector for word 'the' \n",
      "\n",
      "[ 0.08007812  0.10498047  0.04980469  0.0534668  -0.06738281 -0.12060547\n",
      "  0.03515625 -0.11865234  0.04394531  0.03015137 -0.05688477 -0.07617188\n",
      "  0.01287842  0.04980469 -0.08496094 -0.06347656  0.00628662 -0.04321289\n",
      "  0.02026367  0.01330566 -0.01953125  0.09277344 -0.171875   -0.00131989\n",
      "  0.06542969  0.05834961 -0.08251953  0.0859375  -0.00318909  0.05859375\n",
      " -0.03491211 -0.0123291  -0.0480957  -0.00302124  0.05639648  0.01495361\n",
      " -0.07226562 -0.05224609  0.09667969  0.04296875 -0.03540039 -0.07324219\n",
      "  0.03271484 -0.06176758  0.00787354  0.0035553  -0.00878906  0.0390625\n",
      "  0.03833008  0.04443359  0.06982422  0.01263428 -0.00445557 -0.03320312\n",
      " -0.04272461  0.09765625 -0.02160645 -0.0378418   0.01190186 -0.01391602\n",
      " -0.11328125  0.09326172 -0.03930664 -0.11621094  0.02331543 -0.01599121\n",
      "  0.02636719  0.10742188 -0.00466919  0.09619141  0.0279541  -0.05395508\n",
      "  0.08544922 -0.03686523 -0.02026367 -0.08544922  0.125       0.14453125\n",
      "  0.0267334   0.15039062  0.05273438 -0.18652344  0.08154297 -0.01062012\n",
      " -0.03735352 -0.07324219 -0.07519531  0.03613281 -0.13183594  0.00616455\n",
      "  0.05078125  0.04516602  0.0100708  -0.15039062 -0.06005859  0.05761719\n",
      " -0.00692749  0.01586914 -0.0213623   0.10351562 -0.00029182 -0.046875\n",
      " -0.01635742 -0.07861328 -0.06933594  0.01635742 -0.03149414 -0.01373291\n",
      " -0.03662109 -0.08886719 -0.0480957  -0.01318359 -0.07177734  0.00588989\n",
      " -0.04614258  0.03979492  0.10058594 -0.04931641  0.07568359  0.03881836\n",
      " -0.16699219 -0.09619141 -0.10107422  0.02905273 -0.05786133 -0.01928711\n",
      " -0.04296875 -0.08398438 -0.01989746  0.05151367  0.00848389 -0.03613281\n",
      " -0.14941406 -0.01855469 -0.03637695 -0.07666016 -0.03955078 -0.06152344\n",
      " -0.02001953  0.04150391  0.03686523 -0.07226562  0.00592041 -0.06298828\n",
      "  0.00738525 -0.01586914  0.01611328 -0.01452637  0.00772095  0.10107422\n",
      " -0.00558472  0.01428223 -0.07617188  0.05639648 -0.01293945  0.03063965\n",
      " -0.02490234 -0.09863281  0.0324707  -0.02807617 -0.08105469  0.02062988\n",
      "  0.01611328 -0.04199219 -0.03491211 -0.03759766  0.05493164  0.01373291\n",
      "  0.02685547 -0.05859375 -0.07177734 -0.12011719 -0.02282715 -0.1640625\n",
      " -0.00361633 -0.05981445  0.07080078 -0.07714844  0.05175781 -0.04296875\n",
      " -0.04833984  0.0300293  -0.06591797 -0.03173828 -0.04882812 -0.03491211\n",
      "  0.05883789 -0.01464844  0.18066406  0.05688477  0.05249023  0.05786133\n",
      "  0.11669922  0.05200195 -0.0534668   0.01867676 -0.015625    0.00576782\n",
      " -0.07324219 -0.11621094  0.04052734  0.0625     -0.04321289  0.01055908\n",
      "  0.02172852  0.04248047  0.03271484  0.04418945  0.05761719  0.02612305\n",
      " -0.01831055 -0.02697754 -0.00674438  0.00509644 -0.11621094  0.00364685\n",
      "  0.05761719 -0.05957031 -0.08837891  0.0135498   0.04541016 -0.04638672\n",
      " -0.0177002  -0.0625      0.03442383 -0.02416992  0.03088379  0.09570312\n",
      "  0.07958984  0.03930664  0.0279541  -0.0859375   0.08105469  0.06640625\n",
      " -0.00041962 -0.06933594  0.03588867 -0.03417969  0.04492188 -0.00772095\n",
      " -0.00741577 -0.04760742  0.01397705 -0.09960938  0.0246582  -0.09960938\n",
      "  0.11474609  0.03173828  0.02209473  0.07226562  0.03686523  0.02563477\n",
      "  0.01367188 -0.02734375  0.00592041 -0.06738281  0.05053711 -0.02832031\n",
      " -0.04516602 -0.01733398  0.02111816  0.03515625 -0.04296875  0.06640625\n",
      "  0.12207031  0.12353516  0.0039978   0.04516602 -0.01855469  0.04833984\n",
      "  0.04516602  0.08691406  0.02941895  0.03759766  0.03442383 -0.07373047\n",
      " -0.0402832  -0.14648438 -0.02441406 -0.01953125  0.0065918  -0.0018158\n",
      " -0.01092529  0.09326172  0.06542969  0.01843262 -0.09326172 -0.01574707\n",
      " -0.07128906 -0.08935547 -0.07128906 -0.03015137 -0.01300049  0.01635742\n",
      " -0.01831055  0.01483154  0.00500488  0.00366211  0.04760742 -0.06884766]\n",
      "Vocabulary doesn't include word 'a'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# My desktop path: /home/jalaj/MLdataset/models\n",
    "# My laptop path : /home/jalaj/ML_datasets/models/\n",
    "w =  models.Word2Vec.load_word2vec_format('/home/jalaj/MLdataset/models/GoogleNews-vectors-negative300.bin', binary=True)\n",
    "if 'the' in w.wv.vocab:\n",
    "    print (\"Vector for word 'the' \\n\")\n",
    "    print (w.wv['the'])\n",
    "else:\n",
    "    print (\"Vocabulary doesn't include word 'the'\\n\")\n",
    "if 'a' in w.wv.vocab:\n",
    "    print (\"Vector for word 'a' \\n\")\n",
    "    print (w.wv['a'])\n",
    "else:\n",
    "    print (\"Vocabulary doesn't include word 'a'\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Glove example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from gensim.models.word2vec import Text8Corpus\n",
    "from glove import Corpus, Glove"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download text8 corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2018-07-02 20:15:16--  http://mattmahoney.net/dc/text8.zip\n",
      "Resolving mattmahoney.net (mattmahoney.net)... 67.195.197.75\n",
      "Connecting to mattmahoney.net (mattmahoney.net)|67.195.197.75|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 31344016 (30M) [application/zip]\n",
      "Saving to: ‘/tmp/text8.zip’\n",
      "\n",
      "text8.zip           100%[===================>]  29.89M   134KB/s    in 4m 13s  \n",
      "\n",
      "2018-07-02 20:19:30 (121 KB/s) - ‘/tmp/text8.zip’ saved [31344016/31344016]\n",
      "\n",
      "Archive:  /tmp/text8.zip\n",
      "  inflating: text8                   \n"
     ]
    }
   ],
   "source": [
    "# for installing text8 corpus you should follow this commands\n",
    "\n",
    "!wget http://mattmahoney.net/dc/text8.zip -P /tmp\n",
    "!unzip /tmp/text8.zip\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get glove vector for words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing 30 training epochs with 4 threads\n",
      "Epoch 0\n",
      "Epoch 1\n",
      "Epoch 2\n",
      "Epoch 3\n",
      "Epoch 4\n",
      "Epoch 5\n",
      "Epoch 6\n",
      "Epoch 7\n",
      "Epoch 8\n",
      "Epoch 9\n",
      "Epoch 10\n",
      "Epoch 11\n",
      "Epoch 12\n",
      "Epoch 13\n",
      "Epoch 14\n",
      "Epoch 15\n",
      "Epoch 16\n",
      "Epoch 17\n",
      "Epoch 18\n",
      "Epoch 19\n",
      "Epoch 20\n",
      "Epoch 21\n",
      "Epoch 22\n",
      "Epoch 23\n",
      "Epoch 24\n",
      "Epoch 25\n",
      "Epoch 26\n",
      "Epoch 27\n",
      "Epoch 28\n",
      "Epoch 29\n",
      "[('snake', 0.6951705198957675), ('shark', 0.6928492449448447), ('swan', 0.6632405395614013), ('cornet', 0.661971895891401), ('malinois', 0.6587215052820103), ('serpent', 0.656309142561757), ('dome', 0.6553285549419404), ('cherry', 0.651204499700652), ('panda', 0.6502478788622648)]\n",
      "[('young', 0.7822812939693222), ('man', 0.7590825162885275), ('baby', 0.7232012934261004), ('woman', 0.7166125533337554), ('wise', 0.7040130249515133), ('wolf', 0.7031316767439232), ('girls', 0.694375040593598), ('spider', 0.6769184440753099), ('teenage', 0.6695090281036872)]\n",
      "[('driver', 0.889604776022257), ('race', 0.8523965808942278), ('taxi', 0.784547941545665), ('cars', 0.7224222375009319), ('racing', 0.704281144893466), ('accident', 0.6864032346241742), ('touring', 0.6789199596233839), ('crash', 0.664334974799146), ('truck', 0.6355830719036055)]\n"
     ]
    }
   ],
   "source": [
    "#sentences = list(itertools.islice(Text8Corpus('/tmp/text8'), None))\n",
    "sentences = list(itertools.islice(Text8Corpus('./text8'), None))\n",
    "corpus = Corpus()\n",
    "corpus.fit(sentences, window=10)\n",
    "glove = Glove(no_components=100, learning_rate=0.05)\n",
    "glove.fit(corpus.matrix, epochs=30, no_threads=4, verbose=True)\n",
    "glove.add_dictionary(corpus.dictionary)\n",
    "\n",
    "print (glove.most_similar('frog', number=10))\n",
    "print (glove.most_similar('girl', number=10))\n",
    "print (glove.most_similar('car', number=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doc2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python example to train doc2vec model (with or without pre-trained word embeddings)\n",
    "import logging\n",
    "import gensim.models as g\n",
    "import codecs\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc2vec parameters\n",
    "vector_size = 300\n",
    "window_size = 15\n",
    "min_count = 1\n",
    "sampling_threshold = 1e-5\n",
    "negative_size = 5\n",
    "train_epoch = 100\n",
    "dm = 0  # 0 = dbow; 1 = dmpv\n",
    "worker_count = 1  # number of parallel processes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-07-02 20:41:15,718 : INFO : collecting all words and their counts\n",
      "2018-07-02 20:41:15,728 : INFO : PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags\n",
      "2018-07-02 20:41:15,749 : INFO : collected 11097 word types and 1000 unique tags from a corpus of 1000 examples and 84408 words\n",
      "2018-07-02 20:41:15,750 : INFO : Loading a fresh vocabulary\n",
      "2018-07-02 20:41:15,887 : INFO : min_count=1 retains 11097 unique words (100% of original 11097, drops 0)\n",
      "2018-07-02 20:41:15,887 : INFO : min_count=1 leaves 84408 word corpus (100% of original 84408, drops 0)\n",
      "2018-07-02 20:41:15,913 : INFO : deleting the raw counts dictionary of 11097 items\n",
      "2018-07-02 20:41:15,916 : INFO : sample=1e-05 downsamples 3599 most-common words\n",
      "2018-07-02 20:41:15,916 : INFO : downsampling leaves estimated 22704 word corpus (26.9% of prior 84408)\n",
      "2018-07-02 20:41:15,916 : INFO : estimated required memory for 11097 words and 300 dimensions: 33381300 bytes\n",
      "2018-07-02 20:41:15,939 : INFO : resetting layer weights\n",
      "2018-07-02 20:41:16,047 : INFO : training model with 1 workers on 11098 vocabulary and 300 features, using sg=1 hs=0 sample=1e-05 negative=5 window=15\n",
      "2018-07-02 20:41:16,048 : INFO : expecting 1000 sentences, matching count from corpus used for vocabulary survey\n",
      "2018-07-02 20:41:17,087 : INFO : PROGRESS: at 1.76% examples, 40161 words/s, in_qsize 1, out_qsize 0\n",
      "2018-07-02 20:41:18,135 : INFO : PROGRESS: at 3.65% examples, 41463 words/s, in_qsize 1, out_qsize 0\n",
      "2018-07-02 20:41:19,144 : INFO : PROGRESS: at 5.43% examples, 41473 words/s, in_qsize 1, out_qsize 0\n",
      "2018-07-02 20:41:20,185 : INFO : PROGRESS: at 7.29% examples, 41777 words/s, in_qsize 1, out_qsize 0\n",
      "2018-07-02 20:41:21,236 : INFO : PROGRESS: at 9.14% examples, 41874 words/s, in_qsize 1, out_qsize 0\n",
      "2018-07-02 20:41:22,246 : INFO : PROGRESS: at 10.96% examples, 41794 words/s, in_qsize 2, out_qsize 0\n",
      "2018-07-02 20:41:23,267 : INFO : PROGRESS: at 12.71% examples, 41679 words/s, in_qsize 1, out_qsize 0\n",
      "2018-07-02 20:41:24,329 : INFO : PROGRESS: at 14.48% examples, 41411 words/s, in_qsize 1, out_qsize 0\n",
      "2018-07-02 20:41:25,374 : INFO : PROGRESS: at 16.23% examples, 41223 words/s, in_qsize 1, out_qsize 0\n",
      "2018-07-02 20:41:26,378 : INFO : PROGRESS: at 18.00% examples, 41244 words/s, in_qsize 1, out_qsize 0\n",
      "2018-07-02 20:41:27,386 : INFO : PROGRESS: at 19.64% examples, 41025 words/s, in_qsize 1, out_qsize 0\n",
      "2018-07-02 20:41:28,388 : INFO : PROGRESS: at 21.43% examples, 41089 words/s, in_qsize 1, out_qsize 0\n",
      "2018-07-02 20:41:29,400 : INFO : PROGRESS: at 23.14% examples, 41087 words/s, in_qsize 1, out_qsize 0\n",
      "2018-07-02 20:41:30,409 : INFO : PROGRESS: at 24.83% examples, 40927 words/s, in_qsize 2, out_qsize 0\n",
      "2018-07-02 20:41:31,424 : INFO : PROGRESS: at 26.59% examples, 40961 words/s, in_qsize 1, out_qsize 0\n",
      "2018-07-02 20:41:32,485 : INFO : PROGRESS: at 28.48% examples, 41033 words/s, in_qsize 1, out_qsize 0\n",
      "2018-07-02 20:41:33,540 : INFO : PROGRESS: at 30.36% examples, 41101 words/s, in_qsize 1, out_qsize 0\n",
      "2018-07-02 20:41:34,596 : INFO : PROGRESS: at 32.10% examples, 41005 words/s, in_qsize 2, out_qsize 0\n",
      "2018-07-02 20:41:35,636 : INFO : PROGRESS: at 33.89% examples, 40964 words/s, in_qsize 2, out_qsize 0\n",
      "2018-07-02 20:41:36,661 : INFO : PROGRESS: at 35.65% examples, 40966 words/s, in_qsize 1, out_qsize 0\n",
      "2018-07-02 20:41:37,676 : INFO : PROGRESS: at 37.43% examples, 40984 words/s, in_qsize 1, out_qsize 0\n",
      "2018-07-02 20:41:38,679 : INFO : PROGRESS: at 39.14% examples, 41003 words/s, in_qsize 1, out_qsize 0\n",
      "2018-07-02 20:41:39,687 : INFO : PROGRESS: at 40.96% examples, 41019 words/s, in_qsize 1, out_qsize 0\n",
      "2018-07-02 20:41:40,748 : INFO : PROGRESS: at 42.83% examples, 41063 words/s, in_qsize 1, out_qsize 0\n",
      "2018-07-02 20:41:41,806 : INFO : PROGRESS: at 44.48% examples, 40907 words/s, in_qsize 1, out_qsize 0\n",
      "2018-07-02 20:41:42,830 : INFO : PROGRESS: at 46.23% examples, 40900 words/s, in_qsize 1, out_qsize 0\n",
      "2018-07-02 20:41:43,894 : INFO : PROGRESS: at 48.00% examples, 40840 words/s, in_qsize 1, out_qsize 0\n",
      "2018-07-02 20:41:44,909 : INFO : PROGRESS: at 49.54% examples, 40665 words/s, in_qsize 1, out_qsize 0\n",
      "2018-07-02 20:41:45,927 : INFO : PROGRESS: at 51.29% examples, 40673 words/s, in_qsize 1, out_qsize 0\n",
      "2018-07-02 20:41:46,991 : INFO : PROGRESS: at 53.14% examples, 40712 words/s, in_qsize 1, out_qsize 0\n",
      "2018-07-02 20:41:48,038 : INFO : PROGRESS: at 54.96% examples, 40690 words/s, in_qsize 1, out_qsize 0\n",
      "2018-07-02 20:41:49,081 : INFO : PROGRESS: at 56.48% examples, 40511 words/s, in_qsize 2, out_qsize 0\n",
      "2018-07-02 20:41:50,114 : INFO : PROGRESS: at 58.10% examples, 40421 words/s, in_qsize 1, out_qsize 0\n",
      "2018-07-02 20:41:51,140 : INFO : PROGRESS: at 59.76% examples, 40348 words/s, in_qsize 1, out_qsize 0\n",
      "2018-07-02 20:41:52,181 : INFO : PROGRESS: at 61.43% examples, 40284 words/s, in_qsize 2, out_qsize 0\n",
      "2018-07-02 20:41:53,208 : INFO : PROGRESS: at 63.29% examples, 40358 words/s, in_qsize 1, out_qsize 0\n",
      "2018-07-02 20:41:54,274 : INFO : PROGRESS: at 65.04% examples, 40326 words/s, in_qsize 2, out_qsize 0\n",
      "2018-07-02 20:41:55,278 : INFO : PROGRESS: at 66.72% examples, 40290 words/s, in_qsize 2, out_qsize 0\n",
      "2018-07-02 20:41:56,305 : INFO : PROGRESS: at 68.48% examples, 40308 words/s, in_qsize 2, out_qsize 0\n",
      "2018-07-02 20:41:57,369 : INFO : PROGRESS: at 70.10% examples, 40211 words/s, in_qsize 1, out_qsize 0\n",
      "2018-07-02 20:41:58,445 : INFO : PROGRESS: at 71.89% examples, 40178 words/s, in_qsize 2, out_qsize 0\n",
      "2018-07-02 20:41:59,512 : INFO : PROGRESS: at 73.54% examples, 40099 words/s, in_qsize 2, out_qsize 0\n",
      "2018-07-02 20:42:00,525 : INFO : PROGRESS: at 75.04% examples, 39995 words/s, in_qsize 1, out_qsize 0\n",
      "2018-07-02 20:42:01,542 : INFO : PROGRESS: at 76.83% examples, 40020 words/s, in_qsize 1, out_qsize 0\n",
      "2018-07-02 20:42:02,567 : INFO : PROGRESS: at 78.59% examples, 40040 words/s, in_qsize 1, out_qsize 0\n",
      "2018-07-02 20:42:03,573 : INFO : PROGRESS: at 80.36% examples, 40074 words/s, in_qsize 1, out_qsize 0\n",
      "2018-07-02 20:42:04,603 : INFO : PROGRESS: at 82.10% examples, 40082 words/s, in_qsize 1, out_qsize 0\n",
      "2018-07-02 20:42:05,622 : INFO : PROGRESS: at 83.89% examples, 40104 words/s, in_qsize 1, out_qsize 0\n",
      "2018-07-02 20:42:06,676 : INFO : PROGRESS: at 85.64% examples, 40101 words/s, in_qsize 1, out_qsize 0\n",
      "2018-07-02 20:42:07,686 : INFO : PROGRESS: at 87.43% examples, 40131 words/s, in_qsize 1, out_qsize 0\n",
      "2018-07-02 20:42:08,772 : INFO : PROGRESS: at 89.14% examples, 40092 words/s, in_qsize 2, out_qsize 0\n",
      "2018-07-02 20:42:09,791 : INFO : PROGRESS: at 90.96% examples, 40111 words/s, in_qsize 2, out_qsize 0\n",
      "2018-07-02 20:42:10,807 : INFO : PROGRESS: at 92.72% examples, 40133 words/s, in_qsize 2, out_qsize 0\n",
      "2018-07-02 20:42:11,830 : INFO : PROGRESS: at 94.48% examples, 40154 words/s, in_qsize 2, out_qsize 0\n",
      "2018-07-02 20:42:12,849 : INFO : PROGRESS: at 96.23% examples, 40166 words/s, in_qsize 1, out_qsize 0\n",
      "2018-07-02 20:42:13,912 : INFO : PROGRESS: at 98.10% examples, 40198 words/s, in_qsize 1, out_qsize 0\n",
      "2018-07-02 20:42:14,923 : INFO : PROGRESS: at 99.89% examples, 40220 words/s, in_qsize 1, out_qsize 0\n",
      "2018-07-02 20:42:14,993 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-07-02 20:42:14,994 : INFO : training on 8440800 raw words (2370696 effective words) took 58.9s, 40219 effective words/s\n",
      "2018-07-02 20:42:14,994 : INFO : saving Doc2Vec object under ./doc2vecdata/model_new.bin, separately None\n",
      "2018-07-02 20:42:14,995 : INFO : not storing attribute syn0norm\n",
      "2018-07-02 20:42:14,996 : INFO : not storing attribute cum_table\n",
      "2018-07-02 20:42:15,160 : INFO : saved ./doc2vecdata/model_new.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training is over....!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# pretrained word embeddings\n",
    "pretrained_emb = \"./doc2vecdata/pretrained_word_embeddings.txt\"\n",
    "\n",
    "# None if use without pretrained embeddings\n",
    "\n",
    "# input corpus\n",
    "train_corpus = \"./doc2vecdata/train_docs.txt\"\n",
    "\n",
    "# output model\n",
    "saved_path = \"./doc2vecdata/model_new.bin\"\n",
    "\n",
    "# enable logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "# train doc2vec model\n",
    "docs = g.doc2vec.TaggedLineDocument(train_corpus)\n",
    "model = g.Doc2Vec(docs, size=vector_size, window=window_size, min_count=min_count, sample=sampling_threshold,\n",
    "                  workers=worker_count, hs=0, dm=dm, negative=negative_size, dbow_words=1, dm_concat=1,\n",
    "                  iter=train_epoch)\n",
    "\n",
    "# save model\n",
    "model.save(saved_path)\n",
    "\n",
    "print (\"training is over....!\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-07-02 20:50:44,083 : INFO : loading Doc2Vec object from ./doc2vecdata/model_new.bin\n",
      "2018-07-02 20:50:44,238 : INFO : loading docvecs recursively from ./doc2vecdata/model_new.bin.docvecs.* with mmap=None\n",
      "2018-07-02 20:50:44,239 : INFO : loading wv recursively from ./doc2vecdata/model_new.bin.wv.* with mmap=None\n",
      "2018-07-02 20:50:44,239 : INFO : setting ignored attribute syn0norm to None\n",
      "2018-07-02 20:50:44,240 : INFO : setting ignored attribute cum_table to None\n",
      "2018-07-02 20:50:44,240 : INFO : loaded ./doc2vecdata/model_new.bin\n",
      "2018-07-02 20:50:44,256 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing started....!\n",
      "\n",
      "[('clingstone', 0.7625056505203247), ('tow', 0.7624657154083252), ('harmonica', 0.7614538073539734), ('peach', 0.7606684565544128), ('dragonfly', 0.7598357796669006), ('feeder', 0.7598147392272949), ('bag', 0.7591570615768433), ('plum', 0.7586159110069275), ('harp', 0.7567747235298157), ('andirons', 0.756672203540802)]\n"
     ]
    }
   ],
   "source": [
    "print (\"testing started....!\\n\")\n",
    "#parameters\n",
    "model=\"./doc2vecdata/model_new.bin\"\n",
    "test_docs=\"./doc2vecdata/test_docs.txt\"\n",
    "output_file=\"./test_vectors_new.txt\"\n",
    "\n",
    "#inference hyper-parameters\n",
    "start_alpha=0.01\n",
    "infer_epoch=1000\n",
    "\n",
    "#load model\n",
    "m = g.Doc2Vec.load(model)\n",
    "print (m.wv.most_similar(positive=['family', 'dog']))\n",
    "test_docs = [ x.strip().split() for x in codecs.open(test_docs, \"r\", \"utf-8\").readlines() ]\n",
    "\n",
    "#infer test vectors\n",
    "output = open(output_file, \"w\")\n",
    "for d in test_docs:\n",
    "    output.write( \" \".join([str(x) for x in m.infer_vector(d, alpha=start_alpha, steps=infer_epoch)]) + \"\\n\" )\n",
    "output.flush()\n",
    "output.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
